{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39585,"databundleVersionId":4786639,"sourceType":"competition"}],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom IPython.display import display, Markdown\nfrom pathlib import Path\n\ndata_dir = Path('/kaggle/input/learning-equality-curriculum-recommendations')","metadata":{"execution":{"iopub.status.busy":"2024-07-28T09:12:47.248731Z","iopub.execute_input":"2024-07-28T09:12:47.249091Z","iopub.status.idle":"2024-07-28T09:12:47.274597Z","shell.execute_reply.started":"2024-07-28T09:12:47.249010Z","shell.execute_reply":"2024-07-28T09:12:47.273795Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# load the data into pandas dataframes\ntopics_df = pd.read_csv(data_dir / \"topics.csv\", index_col=0).fillna({\"title\": \"\", \"description\": \"\"})\ncontent_df = pd.read_csv(data_dir / \"content.csv\", index_col=0).fillna(\"\")\ncorrelations_df = pd.read_csv(data_dir / \"correlations.csv\", index_col=0)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T09:13:00.937522Z","iopub.execute_input":"2024-07-28T09:13:00.938482Z","iopub.status.idle":"2024-07-28T09:13:21.789403Z","shell.execute_reply.started":"2024-07-28T09:13:00.938428Z","shell.execute_reply":"2024-07-28T09:13:21.788610Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# define some helper functions and classes to aid with data traversal\n\ndef print_markdown(md):\n    display(Markdown(md))\n\nclass Topic:\n    def __init__(self, topic_id):\n        self.id = topic_id\n\n    @property\n    def parent(self):\n        parent_id = topics_df.loc[self.id].parent\n        if pd.isna(parent_id):\n            return None\n        else:\n            return Topic(parent_id)\n\n    @property\n    def ancestors(self):\n        ancestors = []\n        parent = self.parent\n        while parent is not None:\n            ancestors.append(parent)\n            parent = parent.parent\n        return ancestors\n\n    @property\n    def siblings(self):\n        if not self.parent:\n            return []\n        else:\n            return [topic for topic in self.parent.children if topic != self]\n\n    @property\n    def content(self):\n        if self.id in correlations_df.index:\n            return [ContentItem(content_id) for content_id in correlations_df.loc[self.id].content_ids.split()]\n        else:\n            return tuple([]) if self.has_content else []\n\n    def get_breadcrumbs(self, separator=\" >> \", include_self=True, include_root=True):\n        ancestors = self.ancestors\n        if include_self:\n            ancestors = [self] + ancestors\n        if not include_root:\n            ancestors = ancestors[:-1]\n        return separator.join(reversed([a.title for a in ancestors]))\n\n    @property\n    def children(self):\n        return [Topic(child_id) for child_id in topics_df[topics_df.parent == self.id].index]\n\n    def subtree_markdown(self, depth=0):\n        markdown = \"  \" * depth + \"- \" + self.title + \"\\n\"\n        for child in self.children:\n            markdown += child.subtree_markdown(depth=depth + 1)\n        for content in self.content:\n            markdown += (\"  \" * (depth + 1) + \"- \" + \"[\" + content.kind.title() + \"] \" + content.title) + \"\\n\"\n        return markdown\n\n    def __eq__(self, other):\n        if not isinstance(other, Topic):\n            return False\n        return self.id == other.id\n\n    def __getattr__(self, name):\n        return topics_df.loc[self.id][name]\n\n    def __str__(self):\n        return self.title\n    \n    def __repr__(self):\n        return f\"<Topic(id={self.id}, title=\\\"{self.title}\\\")>\"\n\n\nclass ContentItem:\n    def __init__(self, content_id):\n        self.id = content_id\n\n    @property\n    def topics(self):\n        return [Topic(topic_id) for topic_id in topics_df.loc[correlations_df[correlations_df.content_ids.str.contains(self.id)].index].index]\n\n    def __getattr__(self, name):\n        return content_df.loc[self.id][name]\n\n    def __str__(self):\n        return self.title\n    \n    def __repr__(self):\n        return f\"<ContentItem(id={self.id}, title=\\\"{self.title}\\\")>\"\n\n    def __eq__(self, other):\n        if not isinstance(other, ContentItem):\n            return False\n        return self.id == other.id\n\n    def get_all_breadcrumbs(self, separator=\" >> \", include_root=True):\n        breadcrumbs = []\n        for topic in self.topics:\n            new_breadcrumb = topic.get_breadcrumbs(separator=separator, include_root=include_root)\n            if new_breadcrumb:\n                new_breadcrumb = new_breadcrumb + separator + self.title\n            else:\n                new_breadcrumb = self.title\n            breadcrumbs.append(new_breadcrumb)\n        return breadcrumbs","metadata":{"execution":{"iopub.status.busy":"2024-07-28T09:13:35.644741Z","iopub.execute_input":"2024-07-28T09:13:35.645650Z","iopub.status.idle":"2024-07-28T09:13:35.669536Z","shell.execute_reply.started":"2024-07-28T09:13:35.645613Z","shell.execute_reply":"2024-07-28T09:13:35.668395Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Naive inference example\n\nAs a naive example of how we might approach the problem, let's just get a raw embedding of the\ntopic's breadcrumbs and match those to the content titles (using a pretrained multilingual model).","metadata":{}},{"cell_type":"code","source":"# setup and imports\n\n!pip install tqdm sentence_transformers\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm.notebook import tqdm\nfrom annoy import AnnoyIndex\n\nmodel = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T09:24:17.210498Z","iopub.execute_input":"2024-07-28T09:24:17.211129Z","iopub.status.idle":"2024-07-28T09:25:26.982439Z","shell.execute_reply.started":"2024-07-28T09:24:17.211090Z","shell.execute_reply":"2024-07-28T09:25:26.981416Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.0)\nCollecting sentence_transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (4.20.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (1.11.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (0.12.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (1.21.6)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (1.0.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (1.7.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (3.7)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (0.1.97)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (0.10.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.7.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.13.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->sentence_transformers) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->sentence_transformers) (8.0.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence_transformers) (9.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.12)\nBuilding wheels for collected packages: sentence_transformers\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=09a05521dbaed263f3b29bfa1de4ef952f905b4de18516d7e8ae711600562e3f\n  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\nSuccessfully built sentence_transformers\nInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-2.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/744 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b6cc5a5ac064ed4845a57bd1c283540"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5f291ecce8343c1a3a30e1dedb69b09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/114 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e2d8a2d96024ee58e92985330b101ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.58M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"775cece57eb3444ab6e2819a652987b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.58M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48e5165c85114e83b424ce773e0710ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c470eaf171a472d93e5df5f40b23cc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/610 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f49eb02e09514adc91a2ff59b7faa1d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50d7631434af48528f8255a0750ef16c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/539M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7f581c3c321409d88c46f26a273087b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/539M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75e56d5c192d415a880827636cfb2abc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa2303774d6c4e82b866ef51184014e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0e05f83b7fa42c6ad91f826809bdb5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2afb1985ff0c4778b2a70712d69b7155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/531 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efe9a81570dd43d3b812058b09e3bf79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4be1bc37745f4a64a1dfc539ac13ba91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/341 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40599e4a14e144329fa04dc2c82fd1ac"}},"metadata":{}}]},{"cell_type":"code","source":"# generate the embeddings for the topics and content items\n# note: you'll want to ensure you have a GPU accelerator enabled in your notebook for this\n\nprint(\"Embedding topics...\")\ntopic_ids = topics_df[topics_df.has_content].index\ntopic_embeddings = model.encode([Topic(topic_id).get_breadcrumbs() for topic_id in topic_ids])\n\nprint(\"Embedding content...\")\ncontent_ids = content_df.index\ncontent_embeddings = model.encode([ContentItem(content_id).title for content_id in content_ids])","metadata":{"execution":{"iopub.status.busy":"2024-07-28T09:26:23.235021Z","iopub.execute_input":"2024-07-28T09:26:23.236302Z","iopub.status.idle":"2024-07-28T09:30:10.962022Z","shell.execute_reply.started":"2024-07-28T09:26:23.236259Z","shell.execute_reply":"2024-07-28T09:30:10.960900Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Embedding topics...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1923 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d35afe55e9d40328daaabe012909df1"}},"metadata":{}},{"name":"stdout","text":"Embedding content...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4814 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c9bd6235f3443cfa48d21a65049c8ad"}},"metadata":{}}]},{"cell_type":"code","source":"# train a nearest neighbors model on the content embeddings\nnbrs = NearestNeighbors(n_neighbors=35, algorithm='ball_tree').fit(content_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T09:30:25.166915Z","iopub.execute_input":"2024-07-28T09:30:25.167970Z","iopub.status.idle":"2024-07-28T09:30:50.021960Z","shell.execute_reply.started":"2024-07-28T09:30:25.167930Z","shell.execute_reply":"2024-07-28T09:30:50.020857Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# find the nearest neighbors for a specific sample topic, and calculate performance\n# 使用最近邻方法找到最相关的主题，并计算得分\n# specify the index of the target topic to use\ntopic_index = 7888\n\n# calculate the nearest neighbors for the target topic\ndist, nb = nbrs.kneighbors([topic_embeddings[topic_index]])\ntopic = Topic(topic_ids[topic_index])\nprint(\"Topic:\", topic.get_breadcrumbs())\n\n# get the set of ground truth content IDs correlated to the target topic\ntrue_content_ids = set(correlations_df.loc[topic.id].content_ids.split())\n\n# get the set of content IDs returned by the nearest neighbors model\n# (skipping over any content items where the language does not match)\npred_content_ids = []\nfor cindex in nb[0]:\n    cid = content_ids[cindex]\n    content = ContentItem(cid)\n    if content.language == topic.language:\n        pred_content_ids.append(cid)\n    # else:\n    #     print(\"Skipping content item with mismatched language:\", content.title)\n\n# trim to only the top 20 results\npred_content_ids = set(pred_content_ids[:20])\n\n# display the ground truth and predicted content item titles\nprint(\"True content:\")\nfor cid in true_content_ids:\n    print(\"  \", cid, \"\\t\", ContentItem(cid).title)\nprint(\"Predicted content:\")\nif pred_content_ids:\n    for cid in pred_content_ids:\n        print(\"  \", cid, \"\\t\", ContentItem(cid).title)\nelse:\n    print(\"   [None]\")\n\n# calculate the confusion matrix variables\ntp = len(true_content_ids.intersection(pred_content_ids))\nfp = len(pred_content_ids - true_content_ids)\nfn = len(true_content_ids - pred_content_ids)\n\nprint(\"Ground truth count:\", len(true_content_ids))\nprint(\"Predicted count:\", len(pred_content_ids))\nprint(\"True positives:\", tp)\nprint(\"False positives:\", fp)\n\n# calculate the F2 score\nif tp or (fp and fn):\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f2 = tp / (tp + 0.2 * fp + 0.8*fn)\n    print(\"F2:\", f2)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T09:31:34.566087Z","iopub.execute_input":"2024-07-28T09:31:34.567078Z","iopub.status.idle":"2024-07-28T09:31:34.731616Z","shell.execute_reply.started":"2024-07-28T09:31:34.567039Z","shell.execute_reply":"2024-07-28T09:31:34.730635Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Topic: Khan Academy (Español) >> Matemáticas por grado (Perú) >> 3° Secundaria >> Números >> Reescribir números racionales y operaciones de adición y sustracción\nTrue content:\n   c_7958da2706e4 \t Sumar y restar números racionales\n   c_3b9a21d42875 \t Convertir fracciones a decimales\n   c_724886d72d5d \t Expresiones con números racionales\n   c_5ec419614d8b \t Sumar y restar números racionales: 79% - 79.1 - 58 1/10\n   c_a0cce4a12098 \t Desafío de reescribir decimales como fracciones\n   c_0f009f729211 \t Sumar y restar números racionales: 0.79 - 4/3 - 1/2 + 150%\nPredicted content:\n   c_a88e2237af1d \t IIT JEE Trigonometría peliaguda y álgebra (parte 3)\n   c_7958da2706e4 \t Sumar y restar números racionales\nGround truth count: 6\nPredicted count: 2\nTrue positives: 1\nFalse positives: 1\nF2: 0.1923076923076923\n","output_type":"stream"}]},{"cell_type":"code","source":"# calculate the mean F2 over a random sampling of 500 topics\n\nf2_scores = []\n\nindices = np.random.choice(len(topic_ids), 500, replace=False)\n\nfor topic_index in tqdm(indices):\n\n    # calculate the nearest neighbors for the target topic\n    dist, nb = nbrs.kneighbors([topic_embeddings[topic_index]])\n    topic = Topic(topic_ids[topic_index])\n\n    # get the set of ground truth content IDs correlated to the target topic\n    true_content_ids = set(correlations_df.loc[topic.id].content_ids.split())\n\n    # get the set of content IDs returned by the nearest neighbors model\n    # (skipping over any content items where the language does not match)\n    pred_content_ids = []\n    for cindex in nb[0]:\n        cid = content_ids[cindex]\n        content = ContentItem(cid)\n        if content.language == topic.language:\n            pred_content_ids.append(cid)\n\n    # trim to only the top 20 results\n    pred_content_ids = set(pred_content_ids[-20:])\n\n    # calculate the confusion matrix variables\n    tp = len(true_content_ids.intersection(pred_content_ids))\n    fp = len(pred_content_ids - true_content_ids)\n    fn = len(true_content_ids - pred_content_ids)\n\n    # calculate the F2 score\n    if pred_content_ids:\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n        f2 = tp / (tp + 0.2 * fp + 0.8*fn)\n    else:\n        f2 = 0\n\n    f2_scores.append(f2)\n\nprint(\"Average F2:\", np.mean(f2_scores))","metadata":{"execution":{"iopub.status.busy":"2024-07-28T09:31:42.763913Z","iopub.execute_input":"2024-07-28T09:31:42.764934Z","iopub.status.idle":"2024-07-28T09:32:52.318880Z","shell.execute_reply.started":"2024-07-28T09:31:42.764886Z","shell.execute_reply":"2024-07-28T09:32:52.317894Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b731f58628d4b0cb16e232ad98e4220"}},"metadata":{}},{"name":"stdout","text":"Average F2: 0.0891591852619031\n","output_type":"stream"}]}]}